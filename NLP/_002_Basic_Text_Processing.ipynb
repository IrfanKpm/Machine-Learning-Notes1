{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZoViHAz9v2qWJ175K6oRG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IrfanKpm/machine-learning-diaries/blob/main/NLP/_002_Basic_Text_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz9dNqMt5xPG",
        "outputId": "90a3ff98-51a0-45e3-a9bb-c3c65f243c2c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MsLg6kYnY44n"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import contractions\n",
        "\n",
        "# Load spaCy's English tokenizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Basic Tokenization**"
      ],
      "metadata": {
        "id": "piUxZ8TgbioV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world! How are you today?\" # Tokenizing text with contractions.\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "# Tokenize the text\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BBAaeEIbq03",
        "outputId": "63e07c87-49c3-41f1-c3fb-f6e00aaafdfc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '!', 'How', 'are', 'you', 'today', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is the first sentence. This is the second sentence.\" #  Tokenizing a text with multiple sentences.\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqW1ZL2Ydtgl",
        "outputId": "08ed5fdf-c143-4fa2-dfe1-bd055e54f839"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'the', 'first', 'sentence', '.', 'This', 'is', 'the', 'second', 'sentence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Accessing Tokens and Their Attributes in spaCy**\n"
      ],
      "metadata": {
        "id": "3IEHGpECxCZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text\n",
        "text = \"spaCy makes NLP easy.\"\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "# Access tokens\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, Index: {token.i}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik4_tJ3uxIFv",
        "outputId": "624725ca-23f6-4efe-d349-ffdd6274f656"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: spaCy, Index: 0\n",
            "Token: makes, Index: 1\n",
            "Token: NLP, Index: 2\n",
            "Token: easy, Index: 3\n",
            "Token: ., Index: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New text for testing\n",
        "text = \"New York is vibrant, but the weather is unpredictable. Tourists love the Statue.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate over tokens and print their attributes with numbering\n",
        "for i, token in enumerate(doc, start=1):\n",
        "    print(\"-\" * 75)\n",
        "    print(f\"Token {i}: {token.text}\")\n",
        "    print(f\"Lemma {i}: {token.lemma_}\")\n",
        "    print(f\"POS {i}: {token.pos_}\")\n",
        "    print(f\"Tag {i}: {token.tag_}\")\n",
        "    print(f\"Dependency {i}: {token.dep_}\")\n",
        "    print(f\"Shape {i}: {token.shape_}\")\n",
        "    print(f\"Is Alpha {i}: {token.is_alpha}\")\n",
        "    print(f\"Is Stop Word {i}: {token.is_stop}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMQ6JcgDxjgx",
        "outputId": "a2163ddc-ecdf-4f8e-d76b-ba3e3029858f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------\n",
            "Token 1: New\n",
            "Lemma 1: New\n",
            "POS 1: PROPN\n",
            "Tag 1: NNP\n",
            "Dependency 1: compound\n",
            "Shape 1: Xxx\n",
            "Is Alpha 1: True\n",
            "Is Stop Word 1: False\n",
            "---------------------------------------------------------------------------\n",
            "Token 2: York\n",
            "Lemma 2: York\n",
            "POS 2: PROPN\n",
            "Tag 2: NNP\n",
            "Dependency 2: nsubj\n",
            "Shape 2: Xxxx\n",
            "Is Alpha 2: True\n",
            "Is Stop Word 2: False\n",
            "---------------------------------------------------------------------------\n",
            "Token 3: is\n",
            "Lemma 3: be\n",
            "POS 3: AUX\n",
            "Tag 3: VBZ\n",
            "Dependency 3: ROOT\n",
            "Shape 3: xx\n",
            "Is Alpha 3: True\n",
            "Is Stop Word 3: True\n",
            "---------------------------------------------------------------------------\n",
            "Token 4: vibrant\n",
            "Lemma 4: vibrant\n",
            "POS 4: ADJ\n",
            "Tag 4: JJ\n",
            "Dependency 4: acomp\n",
            "Shape 4: xxxx\n",
            "Is Alpha 4: True\n",
            "Is Stop Word 4: False\n",
            "---------------------------------------------------------------------------\n",
            "Token 5: ,\n",
            "Lemma 5: ,\n",
            "POS 5: PUNCT\n",
            "Tag 5: ,\n",
            "Dependency 5: punct\n",
            "Shape 5: ,\n",
            "Is Alpha 5: False\n",
            "Is Stop Word 5: False\n",
            "---------------------------------------------------------------------------\n",
            "Token 6: but\n",
            "Lemma 6: but\n",
            "POS 6: CCONJ\n",
            "Tag 6: CC\n",
            "Dependency 6: cc\n",
            "Shape 6: xxx\n",
            "Is Alpha 6: True\n",
            "Is Stop Word 6: True\n",
            "---------------------------------------------------------------------------\n",
            "Token 7: the\n",
            "Lemma 7: the\n",
            "POS 7: DET\n",
            "Tag 7: DT\n",
            "Dependency 7: det\n",
            "Shape 7: xxx\n",
            "Is Alpha 7: True\n",
            "Is Stop Word 7: True\n",
            "---------------------------------------------------------------------------\n",
            "Token 8: weather\n",
            "Lemma 8: weather\n",
            "POS 8: NOUN\n",
            "Tag 8: NN\n",
            "Dependency 8: nsubj\n",
            "Shape 8: xxxx\n",
            "Is Alpha 8: True\n",
            "Is Stop Word 8: False\n",
            "---------------------------------------------------------------------------\n",
            "Token 9: is\n",
            "Lemma 9: be\n",
            "POS 9: AUX\n",
            "Tag 9: VBZ\n",
            "Dependency 9: conj\n",
            "Shape 9: xx\n",
            "Is Alpha 9: True\n",
            "Is Stop Word 9: True\n",
            "---------------------------------------------------------------------------\n",
            "Token 10: unpredictable\n",
            "Lemma 10: unpredictable\n",
            "POS 10: ADJ\n",
            "Tag 10: JJ\n",
            "Dependency 10: acomp\n",
            "Shape 10: xxxx\n",
            "Is Alpha 10: True\n",
            "Is Stop Word 10: False\n",
            "---------------------------------------------------------------------------\n",
            "Token 11: .\n",
            "Lemma 11: .\n",
            "POS 11: PUNCT\n",
            "Tag 11: .\n",
            "Dependency 11: punct\n",
            "Shape 11: .\n",
            "Is Alpha 11: False\n",
            "Is Stop Word 11: False\n",
            "---------------------------------------------------------------------------\n",
            "Token 12: Tourists\n",
            "Lemma 12: tourist\n",
            "POS 12: NOUN\n",
            "Tag 12: NNS\n",
            "Dependency 12: nsubj\n",
            "Shape 12: Xxxxx\n",
            "Is Alpha 12: True\n",
            "Is Stop Word 12: False\n",
            "---------------------------------------------------------------------------\n",
            "Token 13: love\n",
            "Lemma 13: love\n",
            "POS 13: VERB\n",
            "Tag 13: VBP\n",
            "Dependency 13: ROOT\n",
            "Shape 13: xxxx\n",
            "Is Alpha 13: True\n",
            "Is Stop Word 13: False\n",
            "---------------------------------------------------------------------------\n",
            "Token 14: the\n",
            "Lemma 14: the\n",
            "POS 14: DET\n",
            "Tag 14: DT\n",
            "Dependency 14: det\n",
            "Shape 14: xxx\n",
            "Is Alpha 14: True\n",
            "Is Stop Word 14: True\n",
            "---------------------------------------------------------------------------\n",
            "Token 15: Statue\n",
            "Lemma 15: Statue\n",
            "POS 15: PROPN\n",
            "Tag 15: NNP\n",
            "Dependency 15: dobj\n",
            "Shape 15: Xxxxx\n",
            "Is Alpha 15: True\n",
            "Is Stop Word 15: False\n",
            "---------------------------------------------------------------------------\n",
            "Token 16: .\n",
            "Lemma 16: .\n",
            "POS 16: PUNCT\n",
            "Tag 16: .\n",
            "Dependency 16: punct\n",
            "Shape 16: .\n",
            "Is Alpha 16: False\n",
            "Is Stop Word 16: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "   # Accessing Sentence Token\n",
        "\n",
        "# Sample text with two sentences\n",
        "text = \"The quick brown fox jumps over the lazy dog. The sun is shining brightly today.\"\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "# Iterate over sentences and print tokens\n",
        "for sent in doc.sents:\n",
        "    print(f\"Sentence: {sent.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfTIB_9zyEuP",
        "outputId": "803c1a23-92d4-4596-c956-c617d3c9e2d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: The quick brown fox jumps over the lazy dog.\n",
            "Sentence: The sun is shining brightly today.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Normalization**"
      ],
      "metadata": {
        "id": "bpP8yZ4AzkVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5hJOx-zxziFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lowercasing**"
      ],
      "metadata": {
        "id": "_SzXKlfMztZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a SIMPLE Example.\"\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "# Convert tokens to lowercase\n",
        "tokens = [token.text.lower() for token in doc]\n",
        "print(\"Lowercased Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t7kzK2Uzv8u",
        "outputId": "1eedd6c7-4356-4ca6-8404-33771251bc05"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercased Tokens: ['this', 'is', 'a', 'simple', 'example', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Removing Stop Words**"
      ],
      "metadata": {
        "id": "ZBmr-VxU1Sj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop words are common words that don't add significant meaning to the text.\n",
        "\n",
        "text = \"This is a SIMPLE Example.\"\n",
        "tokens = [token.text for token in doc if not token.is_stop]\n",
        "print(\"Tokens without Stop Words:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxKQfP5p1VMF",
        "outputId": "4513ad9f-67cb-46ce-f5d6-1bb4807805de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without Stop Words: ['SIMPLE', 'Example', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Expanding Contractions**"
      ],
      "metadata": {
        "id": "YG0DOV6b5nnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text with contractions\n",
        "text = \"I can't go there because it's raining.\"\n",
        "\n",
        "# Expand contractions\n",
        "expanded_text = contractions.fix(text)\n",
        "print(f\"Input Text : {text}\")\n",
        "print(\"Expanded Text : \", expanded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4DJNSLw5rJR",
        "outputId": "74f070c4-7460-4e36-b86d-a031e0c8307f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text : I can't go there because it's raining.\n",
            "Expanded Text :  I cannot go there because it is raining.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Removing Special Characters**"
      ],
      "metadata": {
        "id": "NZ-8pD6b6H-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I can't go there because it's raining.\"\n",
        "doc = nlp(text)\n",
        "cleaned_tokens = [token.text for token in doc if token.is_alpha]\n",
        "print(\"Tokens without Special Characters:\", cleaned_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRflpWsK6JcA",
        "outputId": "38c8eb05-25ad-41d8-d95f-5839c88f0dd0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without Special Characters: ['I', 'ca', 'go', 'there', 'because', 'it', 'raining']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **example tasks**"
      ],
      "metadata": {
        "id": "dRY8f4JF_bMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"\n",
        "John recently moved to New York and started a new job as a software engineer. He’s been exploring the city and enjoying the vibrant culture. If you want to reach out to him for professional inquiries, you can email him at john.doe@example.com. He’s always open to connecting with like-minded professionals.\n",
        "Sophia is an experienced graphic designer who freelances for various international clients. She specializes in branding and web design, often sharing her work on social media. For collaborations, you can contact her at sophia.artwork@example.com. She’s excited to work on new and creative projects that challenge her skills.\n",
        "Michael is a digital marketer who recently launched his own agency. He helps small businesses grow their online presence through strategic marketing. You can get in touch with him at michael.marketing@example.com for consultations. He believes in personalized strategies that drive results and loves working with passionate entrepreneurs.\n",
        "\"\"\"\n",
        "\n",
        "doc = nlp(data)\n",
        "\n",
        "for token in doc:\n",
        "    if token.like_email:\n",
        "        print(f\"Email found: {token.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiKfGNfa_fT3",
        "outputId": "467f4ddc-4449-483c-a3aa-9efe7044940d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email found: john.doe@example.com\n",
            "Email found: sophia.artwork@example.com\n",
            "Email found: michael.marketing@example.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **malayalam tokenization**"
      ],
      "metadata": {
        "id": "NJhB_N8lB7LW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"ml\")\n",
        "\n",
        "# Sample Malayalam text\n",
        "text = \"ഞാൻ എങ്ങനെ സഹായിക്കാമെന്ന് നോക്കാം.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print tokens\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDQvEPjUBoz6",
        "outputId": "e7e67bca-8c3d-4c6a-ca3f-49c96f4186fb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: ഞാൻ\n",
            "Token: എങ്ങനെ\n",
            "Token: സഹായിക്കാമെന്ന്\n",
            "Token: നോക്കാം.\n"
          ]
        }
      ]
    }
  ]
}